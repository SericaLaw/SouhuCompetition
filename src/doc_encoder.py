import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import os
SAVE_INFO_PATH = "./model/saveInfo"


class PassageEncoder(nn.Module):
    def __init__(self, input_size, output_size):
        '''
        initial encoder
        :param input_size: input size of lstm
        :param output_size:  output size of lstm
        '''
        super(PassageEncoder,self).__init__()
        self.bilstm = nn.LSTM(input_size=input_size,hidden_size=output_size,
                              dropout=0.33,bidirectional=True)

    def forward(self, feature, input_length, hidden=None):
        '''
        :param feature: torch tensor of shape(batch_size, )
        :param input_length: length of input, torch tensor of dtype int
        :param hidden: initial hidden state, torch tensor shape(output_size)
        :return: the output of lstm of shape(batch_size, length, output_size)
        '''
        feature = feature.float()
        packed = nn.utils.rnn.pack_padded_sequence(feature, batch_first=True, lengths=input_length)
        lstm_out,hidden = self.bilstm(packed, hidden)
        out = nn.utils.rnn.pad_packed_sequence(lstm_out,batch_first=True)
        return out


def load_model(path, hidden_size=10, version="last"):
    model = PassageEncoder(hidden_size,hidden_size)
    if not os.path.exists(path):
        print("not exist")
        return model, 0
    else:
        saveinfo = torch.load(path)[version]
        model.load_state_dict(torch.load(saveinfo)["model_state_dict"])
        cur_epoch = torch.load(saveinfo["epoch"])
        return model, cur_epoch


def aggregate_title_content(self, title, content):
    '''
    aggregate title and content to form passage vector
    here just cat content after title
    :param title: torch tensor of shape (title_len, d)
    :param content: torch tensor of shape (content_len, d)
    :return: aggregated passage tensor of shape(title_len+content_len, d)
    '''
    passage = torch.cat([title,content],dim=0)
    return passage


def loss_fn(encoded_passage, label, entity_candidate):
    '''
    triplet loss with anchor as encoded_passage, positive as label, negtive as entity_candidate
    :param encoded_passage: output of lstm with shape (batch_size, seq_length, num_layer, output_size)
    :param label: a batch_size length list of dict contain the passage label of 3 entity
    :param entity_candidate: a batch_size length list of dict contain negative entity generated by tf-idf
    :return: loss
    '''
    print(encoded_passage[1,-1,:].shape)
    total_loss = torch.zeros(1,dtype=torch.float)
    for i_batch in range(encoded_passage.shape[0]):
        for entity in label[i_batch]:
            for candidate in entity_candidate[i_batch]:
                total_loss += nn.functional.triplet_margin_loss(
                    anchor=encoded_passage[i_batch][-1,:], positive=entity["encoding"], negative=candidate["encoding"]
                )
    return total_loss


def getdata(dataloader):
    return dataloader


def train(dataloader, model, num_epcoh=1, lr=0.1, save_path=None, cur_epoch=0):
    model = model
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    print("start training")
    for step in range(num_epcoh):
        print("epoch ",cur_epoch)
        # for idx, data in enumerate(dataloader):
        optimizer.zero_grad()
        data = getdata(dataloader)
        passage, length, label, candidate = data["passage"], data["length"], data["entity"], data["candidate"]
        encoded_passage, _ = model(passage, length)
        shape = encoded_passage.shape
        encoded_passage = encoded_passage.view(shape[0],shape[1],2,-1)
        loss = loss_fn(encoded_passage, label, candidate)
        loss.backward()
        optimizer.step()
        cur_epoch += 1
        if save_path:
            torch.save({"model_state_dict": model.state_dict(),"epoch": cur_epoch},save_path)


def eval(dataloader, model):
    model.eval()
    data = getdata(dataloader)
    passage, length, label, candidate = data["passage"], data["length"], data["entity"], data["candidate"]
    encoded_passage, _ = model(passage, length)
    shape = encoded_passage.shape
    encoded_passage = encoded_passage.view(shape[0], shape[1], 2, -1)
    loss = loss_fn(encoded_passage, label, candidate)
    print(loss)


def stackpassage(passages):
    '''

    :param passages:
    :return: passages sorted by length
    '''
    passages.sort(key=lambda x:x["length"],reverse=True)
    dataset = {}
    for key in passages[0]:
        dataset[key] = []
        for p in passages:
            dataset[key].append(p[key])
    print(len(dataset["passage"]), dataset["passage"][0].shape)
    print(dataset["passage"][1].shape)
    dataset["passage"] = nn.utils.rnn.pad_sequence(dataset["passage"],batch_first=True)
    return dataset


def test():
    nd = 10
    passages = []
    for i in range(3):
        passage = {}
        # passage["title"] = torch.randn(size=(1+i,nd))
        # passage["content"] = torch.randn(size=(2+i,nd))
        passage["passage"] = torch.randn(size=(2+i, nd))
        passage["length"] = 2+i
        passage["entity"] = [{"entity":"a", "encoding": torch.randn(size=(nd,))}]
        passage["candidate"] = [{"entity":"a", "encoding": torch.randn(size=(nd,))},
                                {"entity": "c", "encoding": torch.randn(size=(nd,))}]
        passages.append(passage)
    # dataset = PassageDataset(passages)
    # dataloader = DataLoader(dataset,2, True,collate_fn=collate_wrapper, pin_memory=True)

    dataloader = stackpassage(passages)
    model,epoch = load_model(SAVE_INFO_PATH)
    train(dataloader,model,cur_epoch=epoch)


if __name__ == "__main__":
    test()
    # load_model(SAVE_INFO_PATH,10)
