import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


class PassageEncoder(nn.Module):
    def __init__(self, input_size, output_size):
        '''
        initial encoder
        :param input_size: input size of lstm
        :param output_size:  output size of lstm
        '''
        super(PassageEncoder,self).__init__()
        self.bilstm = nn.LSTM(input_size=input_size,hidden_size=output_size,
                              dropout=0.33,bidirectional=True)

    def forward(self, feature, input_length, hidden=None):
        '''
        :param feature: torch tensor of shape(batch_size, )
        :param input_length: length of input, torch tensor of dtype int
        :param hidden: initial hidden state, torch tensor shape(output_size)
        :return: the output of lstm of shape(batch_size, length, output_size)
        '''
        packed = nn.utils.rnn.pack_padded_sequence(feature, batch_first=True)
        lstm_out,hidden = self.bilstm(packed, hidden)
        out = nn.utils.rnn.pad_packed_sequence(lstm_out,batch_first=True)
        return out


class PassageDataset(Dataset):
    def __init__(self, passages):
        '''
        :param passages: a list contain dict with keys title, content, label and entity candidate
                        title and content shape in (d, length)
        '''
        self.all_passage = []
        self.max_length = 0
        for passage in passages:
            title = passage["title"]
            content = passage["content"]
            label = passage["entity"]
            entity_candidata = passage["candidate"]
            passage_vector = self.aggregate_title_content(title,content)
            length = passage_vector.shape[1]
            if self.max_length < length:
                self.max_length = length
            self.all_passage.append({"passage":passage_vector,
                                     "label":label,
                                     "length":length,
                                     "entity_candidate":entity_candidata})

    def __len__(self):
        return len(self.all_passage)

    def __getitem__(self, item):
        passage = self.all_passage[item]
        return passage

    def aggregate_title_content(self, title, content):
        '''
        aggregate title and content to form passage vector
        here just cat content after title
        :param title: torch tensor of shape (d, title_len)
        :param content: torch tensor of shape (d, content_len)
        :return: aggregated passage tensor of shape(d, title_len+content_len)
        '''
        passage = torch.cat([title,content],dim=1)
        return passage


class PaddingBatch:
    def __init__(self, data):
        print("here PaddingBatch")
        # print(type(data),data)
        self.data = list(zip(*data))
        print(self.data)

    def pin_memory(self):
        return self

def collate_wrapper(batch):
    print("wrapper: ", batch)
    print("wrapper len: ", len(batch))
    print("wrapper type:", type(batch))
    return PaddingBatch(batch)

def loss(encoded_passage, label, entity_candidate):
    '''
    triplet loss with anchor as encoded_passage, positive as label, negtive as entity_candidate
    :param encoded_passage: output of lstm with shape ()
    :param label: the passage label of 3 entity
    :param entity_candidate: generated by tf-idf
    :return: loss
    '''
    total_loss = torch.zeros(1,dtype=torch.float)
    for entity in label:
        for candidate in entity_candidate:
            total_loss += nn.functional.triplet_margin_loss(
                anchor=encoded_passage, positive=entity, negtive=candidate
            )
    return total_loss


def train(dataloader, model):
    model.train()
    NUM_EPOCH = 10
    LEARNING_RATE = 0.1
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    for step in range(NUM_EPOCH):
        for idx, data in enumerate(dataloader):
            passage, length, label, candidate = data["passage"], data["length"], data["label"], data["entity_candidate"]
            print(passage.shape)
            print(length.shape)
            print(length)
            data_sort_by_len = torch.sort(length, descending=True)
            print(data_sort_by_len)
            sorted_passage = passage[data_sort_by_len[1].tolist()]
            sorted_label = label[data_sort_by_len[1].tolist()]
            encoded_passage, _ = model(sorted_passage, length)
            loss = loss(encoded_passage, sorted_label, candidate)
            loss.backward()
            optimizer.step()


def test():
    nd = 5
    passages = []
    for i in range(3):
        passage = {}
        passage["title"] = torch.randn(size=(nd, 1+i))
        passage["content"] = torch.randn(size=(nd,2+i))
        passage["entity"] = [{"a":torch.randn(size=(nd,))},
                             {"b":torch.randn(size=(nd,))},
                             {"c": torch.randn(size=(nd,))}]
        passage["candidate"] = [{"a":torch.randn(size=(nd,))},
                                {"e": torch.randn(size=(nd,))},
                                {"b": torch.randn(size=(nd,))},
                                {"f": torch.randn(size=(nd,))}]
        passages.append(passage)
    dataset = PassageDataset(passages)
    dataloader = DataLoader(dataset,2, True,collate_fn=collate_wrapper, pin_memory=True)
    model = PassageEncoder(nd, nd)
    train(dataloader,model)


if __name__ == "__main__":
    test()